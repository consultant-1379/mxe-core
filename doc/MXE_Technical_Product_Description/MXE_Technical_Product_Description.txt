= Technical Product Description
:author: Kristóf Nékám
:signature: EKRINKM
:department: BDGSJBAK
:doc-name: TECHN PRODUCT DESCR
:doc-no: 1/221 02-AVA 901 53
:revnumber: PF2
:revdate: {sys: date +%Y-%m-%d}
:approved-by-name: Attila Ulbert
:approved-by-signature: EATTULB
:approved-by-department: BDGSBEIP

//Template updated 2018-08-21 (keep to track template history)

[[ArchitecturalOverview]]
== Architectural Overview

The following figure shows the architecture of MXE. The individual components are described below.

image::mxe_architecture.png[title="MXE Architecture",scalefit="1"]

*MXE CLI* +
The CLI of MXE offers a comprehensive list of configurable commands that allow the administration of models and flows. For more information, see doc-ref:[CLI Guide].

*Browser* +
MXE supports Chrome, Firefox, and MS Edge, with the latest versions available at the MXE release.

*REST API* +
REST API operations are available in connection with models, flows and their deployments, as an alternative to the MXE CLI.

*Exposed Components* +
Exposed components are accessible through Gatekeeper, and by using the CLI, the GUI, or the REST API. They compose the front end of MXE.

*Gatekeeper* +
Gatekeeper is an adapter which integrates with the Keycloak authentication service supporting both access tokens in browser cookies and bearer tokens.

*Internal Components* +
Internal components are the back end of MXE, and as such, they are not accessible directly.

*Keycloak* +
Keycloak is an identity and access management solution.

*MXE API Services* +
The back end services of MXE, facilitating the handling of models and flows by exposing MXE functions through REST endpoints. For more information, see doc-ref:[REST API].

*MXE Deployments* +
The actual models and flows deployed by the users of MXE.

*UI Components* +
MXE has its own GUI, built using the E-UI SDK.

*Internal 3PPs* +
Third party products used by MXE to facilitate various aspects of the machine learning model handling.

*Ingress* +
Ingress is an object that facilitates external access to the services behind it. It requires an Ingress Controller to work.

*Storage Services* +
Services to store data, models, user accounts and rights, and so on.

[[Concepts]]
== Concepts

[[Models]]
=== Models

The _model_ itself may be considered as a simple function that for a given input returns a single response. In practice this means that the _model code_ must provide a single predict function (method) which will be exposed by Seldon. The model implements initialization in the constructor of the predictor class. An example for this is when the model needs to read files to prepare for serving requests. The model must not have an internal state that changes in-between requests. It cannot write any files or in any other way persist information that affects its behavior later. The model must not access any remote data sources for input or try to implement a scheduling logic on its own. All its input must come from the request. The model code must also be able to process a request within 3 seconds.

MXE uses Seldon core for ML model packaging and serving. Model code must comply with Seldon's requirements, see: https://docs.seldon.io/projects/seldon-core/en/v0.2.7/wrappers/README.html[]

[[ModelidandVersion]]
==== Model id and Version

A specific model is identified by the `id` and `version` of the model. The `id` can only contain lower case alphanumeric characters and dots (for example `com.ericsson.imagerecognition.model1`). The version must follow the format X.Y.Z (where X, Y and Z are non-negative integers), which is the normal version number format known as https://semver.org/[semantic versioning] (for example 1.0.2).

Several versions of a model with the same `id` can be onboarded to MXE.

[[ModelPackaging]]
==== Model Packaging

During model packaging the model source code is encapsulated in a webservice and along with all its dependencies built into a Docker image (called the _model image_). Model packaging is possible without the MXE cluster (through the `mxe-model package` CLI command), or inside the MXE cluster (through the `mxe-model onboard --source` CLI command or the MXE GUI). The result of the model packaging is a model package, containing everything needed for running the model as a service, including model meta information (`id`, `version`, `title`, `author`, `description`, `icon`).

[[ModelOnboarding]]
==== Model Onboarding

Models have to be onboarded to an MXE cluster before being able to actually start using them. During onboarding, the model meta information is recorded into the MXE model package catalogue.

.MXE provides three options for onboarding a model:
* onboard from source files (`mxe-model onboard --source` CLI command or the MXE GUI)
* onboard from model package archive generated by the `mxe-model package` CLI command (`mxe-model onboard --archive` CLI command)
* onboard from an external docker registry, for example https://armdocker.rnd.ericsson.se[] (`mxe-model onboard --docker` CLI command)

[[CreatingModelServices]]
==== Creating Model Services

A model service is created from an onboarded model to be able to serve predictions.

[[ModelServices]]
==== Model Services

A model service is the model deployed as a Cloud Native Microservice, serving prediction requests from the model clients. It has an external REST API endpoint which can be invoked.

The model service has a unique model service name.

[[ModifyingModelServiceInstances]]
==== Modifying Model Service Instances

Behind the scenes a model service directs the incoming requests to a number of containers running the corresponding model code. The number of such _model containers_ is called its instance number. The higher the number of instances the more requests the model service can serve simultaneously.

[[ModifyingModelsInModelServices]]
==== Modifying Models In Model Services

Modifying a model in a model service means that the model is replaced with an other model or an other version of the same model in the model service. To change a model, the new model has to be onboarded first.

[[ModelLifecycleManagement]]
==== Model Lifecycle Management

.The typical steps of a model lifecycle are:
. Develop & train model
. Package model
. Push model Docker image to a Docker registry
. Onboard model
. Create model service
. Modify model service instances (optional)
. Onboard newer version of the model or a different model (optional)
. Modify model service to replace the model with new model or newer version (optional)
. Delete model service
. Delete model

[[ModelIntegrationRelatedConcepts]]
==== Model Integration Related Concepts

Model services in MXE only expose an HTTP interface. Whatever data preparation or preprocessing is required before invoking a model has to be implemented by the application logic that is using it. In the same way, it is the application that has to implement the scheduling of model invocations the model code must not contain such logic. It is a frequent use-case however that a model must be applied directly to data in some storage or stream without much preprocessing. MXE provides a solution for such an integration problem through Apache NiFi. NiFi is an open-source tool for automating and managing the flow of data between systems based on the concepts of flow-based programming. It has out-of-the-box integration with most widespread data storage and streaming technologies and can invoke HTTP.

[[Flows]]
=== Flows

[[IntegrationFlows]]
==== Integration Flows

The internal state of a running NiFi instance is stored in a `flow.xml.gz` file by NiFi. This is what is referred to as a _flow_ in MXE. By injecting such a _flow_ into a new empty NiFi instance it becomes possible to deploy new instances of the same flow. Similarly to models, flows also have to be onboarded to MXE before use.

[[IntegrationFlowDeployments]]
==== Integration Flow Deployments

A _flow deployment_ is a running NiFi instance that was created from an already onboarded flow. The GUI of flow deployments are exposed similarly to model deployments. It is not advised to modify the parameters of processors in a running deployment as these changes would get lost if in case of a pod eviction by Kubernetes.